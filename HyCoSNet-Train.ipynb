{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.status.idle": "2025-08-25T02:57:08.759339Z",
     "shell.execute_reply": "2025-08-25T02:57:08.758384Z",
     "shell.execute_reply.started": "2025-08-25T02:54:55.526353Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel, tqdm]\"\n",
    "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "%matplotlib inline\n",
    "!pip install timm\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T02:57:08.761371Z",
     "iopub.status.busy": "2025-08-25T02:57:08.760584Z",
     "iopub.status.idle": "2025-08-25T02:57:44.110989Z",
     "shell.execute_reply": "2025-08-25T02:57:44.110368Z",
     "shell.execute_reply.started": "2025-08-25T02:57:08.761333Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import monai\n",
    "from monai.apps import DecathlonDataset\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, decollate_batch,ImageDataset,Dataset\n",
    "from monai.handlers.utils import from_engine\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.metrics import DiceMetric,ROCAUCMetric\n",
    "from monai.networks.nets import SegResNet,DenseNet121\n",
    "from monai.optimizers import LearningRateFinder\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    Resized,\n",
    "    RandRotate90d,\n",
    "    CropForegroundd,\n",
    "    ScaleIntensityd,\n",
    "    Activations, \n",
    "    AsDiscrete,\n",
    "    EnsureChannelFirstd,\n",
    "    Rand3DElasticd,\n",
    "    RandRotated,\n",
    "    PadListDataCollate,\n",
    "    RandScaleIntensityd,\n",
    "    SpatialPadd,\n",
    "    RandAffined,\n",
    "    RandFlipd,\n",
    "    Spacingd,\n",
    "    RandAdjustContrastd,\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "import timm \n",
    "from torchvision.models import swin_transformer\n",
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "from collections.abc import Callable, Sequence\n",
    "\n",
    "import torch\n",
    "from torch.hub import load_state_dict_from_url\n",
    "\n",
    "from monai.networks.layers.factories import Conv, Dropout, Pool\n",
    "from monai.networks.layers.utils import get_act_layer, get_norm_layer\n",
    "from monai.utils.module import look_up_option\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import SimpleITK as sitk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T02:57:44.113171Z",
     "iopub.status.busy": "2025-08-25T02:57:44.112480Z",
     "iopub.status.idle": "2025-08-25T02:57:44.198141Z",
     "shell.execute_reply": "2025-08-25T02:57:44.197535Z",
     "shell.execute_reply.started": "2025-08-25T02:57:44.113143Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pin_memory = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "PATH_DATASET = '/kaggle/input/ucsf-seg/segImgs'\n",
    "#df_train = pd.read_csv(('/kaggle/input/ucsf-csv/UCSF-MGMT.csv'),usecols=['ID', 'MGMT'])\n",
    "df_train = pd.read_csv(('/kaggle/input/ucsf-csv/UCSF-os-train_stratified.csv'),usecols=['ID', 'MGMT','IDH','age','gender','WHO CNS Grade','OS','EOR','fold'])\n",
    "images = []\n",
    "weight_decay=0.01\n",
    "auc_metric = ROCAUCMetric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T02:57:44.709721Z",
     "iopub.status.busy": "2025-08-25T02:57:44.709440Z",
     "iopub.status.idle": "2025-08-25T02:57:44.737695Z",
     "shell.execute_reply": "2025-08-25T02:57:44.736966Z",
     "shell.execute_reply.started": "2025-08-25T02:57:44.709698Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Tuple\n",
    "import torch.nn.functional as F  # 添加：用于 F.gelu\n",
    "\n",
    "\n",
    "try:\n",
    "    from monai.networks.nets import SwinUNETR\n",
    "   \n",
    "    try:\n",
    "        from timm.models.layers import DropPath\n",
    "    except ImportError:\n",
    "        from monai.networks.blocks.patchembedding import DropPath\n",
    "    MONAI_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: MONAI or its components (SwinUNETR, DropPath) not found. Using dummy classes.\")\n",
    "    MONAI_AVAILABLE = False\n",
    "    class SwinUNETR:\n",
    "        def __init__(self, *args, **kwargs): pass\n",
    "    class DropPath(nn.Module):\n",
    "        def __init__(self, *args, **kwargs): super().__init__()\n",
    "        def forward(self, x): return x\n",
    "\n",
    "class LayerNormChannelsFirst(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-6):\n",
    "        super().__init__()\n",
    "       \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class ConvNeXtBlock3D(nn.Module):\n",
    "    def __init__(self, dim, layer_scale_init_value=1e-6, drop_path=0.):\n",
    "       \n",
    "    def forward(self, x):\n",
    "\n",
    "        return x\n",
    "\n",
    "# --- CrossAttentionFusionBlock (不变) ---\n",
    "class CrossAttentionFusionBlock(nn.Module):\n",
    "   \n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x_t1c, x_flair):\n",
    "       \n",
    "        return fused_map\n",
    "\n",
    "\n",
    "\n",
    "class HyCoSNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 1,\n",
    "        out_channels: int = 2,  \n",
    "        swin_feature_size: int = 24,\n",
    "        use_checkpoint: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if not MONAI_AVAILABLE:\n",
    "            raise RuntimeError(\"MONAI is not available, cannot create HyCoSNet.\")\n",
    "\n",
    "        self.flair_encoder = SwinUNETR(\n",
    "            in_channels=in_channels, out_channels=out_channels,\n",
    "            feature_size=swin_feature_size, use_checkpoint=use_checkpoint, spatial_dims=3\n",
    "        )\n",
    "\n",
    "       \n",
    "        self.t1c_stem = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, convnext_dims[0], kernel_size=4, stride=4),\n",
    "            LayerNormChannelsFirst(normalized_shape=convnext_dims[0])\n",
    "        )\n",
    "        self.t1c_stage1 = ConvNeXtBlock3D(dim=convnext_dims[0])\n",
    "        \n",
    "\n",
    "    def forward(self, mri_t1c, mri_flair, extra_features=None):\n",
    "        \n",
    "        return logits  # (B, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T02:57:44.738751Z",
     "iopub.status.busy": "2025-08-25T02:57:44.738422Z",
     "iopub.status.idle": "2025-08-25T02:57:44.757809Z",
     "shell.execute_reply": "2025-08-25T02:57:44.757227Z",
     "shell.execute_reply.started": "2025-08-25T02:57:44.738722Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def readIMGs(imgPath, csvFile, imgtype, N):\n",
    "    train_files = [] # 确保列表已初始化\n",
    "\n",
    "    if N == 0:\n",
    "        tfolds = [1, 2, 3, 4]\n",
    "    elif N == 1:\n",
    "        tfolds = [0, 2, 3, 4]\n",
    "    elif N == 2:\n",
    "        tfolds = [0, 1, 3, 4]\n",
    "    elif N == 3:\n",
    "        tfolds = [0, 1, 2, 4]\n",
    "    elif N == 4:\n",
    "        tfolds = [0, 1, 2, 3]\n",
    "    \n",
    "    for _, row in csvFile.iterrows():\n",
    "        id_ = row['ID']\n",
    "        if int(row['fold']) in tfolds:\n",
    "          \n",
    "            id_modified = id_[0:10] + '0' + id_[10:]\n",
    "            img1_path = os.path.join(imgPath, id_modified + imgtype[0] + \"_trans.nii\")\n",
    "            img2_path = os.path.join(imgPath, id_modified + imgtype[1] + \"_trans.nii\")\n",
    "\n",
    "            # 2. 检查这两个文件路径是否都真实存在\n",
    "            if os.path.exists(img1_path) and os.path.exists(img2_path):\n",
    "                # 3. 如果都存在，则添加信息到列表中\n",
    "                train_files.append({\n",
    "                    \"img1\": img1_path,\n",
    "                    \"img2\": img2_path,\n",
    "                    \"age\": row['age'],\n",
    "                    \"gender\": row['gender'],\n",
    "                \n",
    "                    \"label\": row['OS'], \n",
    "                })\n",
    "            else:\n",
    "                # （可选）可以打印更详细的警告\n",
    "                if not os.path.exists(img1_path):\n",
    "                    print(f\"Warning: Image not found for {img1_path}\")\n",
    "                if not os.path.exists(img2_path):\n",
    "                    print(f\"Warning: Image not found for {img2_path}\")\n",
    "\n",
    "   \n",
    "    return train_files\n",
    "\n",
    "def readIMGsTest(imgPath, csvFile, imgtype, foldN):\n",
    "    test_files = [] \n",
    "    \n",
    "    for _, row in csvFile.iterrows():\n",
    "        id_ = row['ID']\n",
    "        if int(row['fold']) == foldN:\n",
    "            id_modified = id_[0:10] + '0' + id_[10:]\n",
    "  \n",
    "            img1_path = os.path.join(imgPath, id_modified + imgtype[0] + \"_trans.nii\")\n",
    "            img2_path = os.path.join(imgPath, id_modified + imgtype[1] + \"_trans.nii\")\n",
    "            \n",
    "            # 检查两个文件是否存在\n",
    "            if os.path.exists(img1_path) and os.path.exists(img2_path):\n",
    "                test_files.append({\n",
    "                    \"img1\": img1_path,\n",
    "                    \"img2\": img2_path,\n",
    "                   \n",
    "                    \"age\": row['age'],\n",
    "                    \"gender\": row['gender'],\n",
    "                   \n",
    "                    \"label\": row['OS'],\n",
    "                })\n",
    "    \n",
    "    # 返回正确的变量名\n",
    "    return test_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T02:57:44.759052Z",
     "iopub.status.busy": "2025-08-25T02:57:44.758796Z",
     "iopub.status.idle": "2025-08-25T02:57:44.778054Z",
     "shell.execute_reply": "2025-08-25T02:57:44.777306Z",
     "shell.execute_reply.started": "2025-08-25T02:57:44.759033Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def transformers():\n",
    "    train_transforms = Compose(\n",
    "    [\n",
    "      \n",
    "        LoadImaged(keys=[\"img1\", \"img2\"]),\n",
    "        EnsureChannelFirstd(keys=[\"img1\", \"img2\"]),\n",
    "        Orientationd(keys=[\"img1\", \"img2\"], axcodes=\"RAS\"),\n",
    "        CropForegroundd(keys=[\"img1\", \"img2\"], source_key=\"img1\", allow_smaller=True),\n",
    "        \n",
    "       \n",
    "        RandFlipd(keys=[\"img1\", \"img2\"], prob=0.3, spatial_axis=0),  \n",
    "        RandFlipd(keys=[\"img1\", \"img2\"], prob=0.3, spatial_axis=1),  \n",
    "        RandRotate90d(keys=[\"img1\", \"img2\"], prob=0.2, spatial_axes=(0, 2)),  \n",
    "        RandAffined(\n",
    "            keys=[\"img1\", \"img2\"],\n",
    "            prob=0.2, \n",
    "            rotate_range=(np.pi / 72, np.pi / 72, np.pi / 72),  \n",
    "            scale_range=(0.05, 0.05, 0.05),  \n",
    "            mode=(\"bilinear\", \"bilinear\"),\n",
    "            padding_mode=\"zeros\"\n",
    "        ),\n",
    "        \n",
    "       \n",
    "        Resized(keys=[\"img1\", \"img2\"], spatial_size=(130, 170, 130)),\n",
    "        SpatialPadd(keys=[\"img1\", \"img2\"], spatial_size=(130, 170, 130)), \n",
    "        \n",
    "      \n",
    "        ScaleIntensityd(keys=[\"img1\", \"img2\"]),\n",
    "        RandScaleIntensityd(keys=[\"img1\", \"img2\"], factors=0.05, prob=0.3), \n",
    "        RandAdjustContrastd(keys=[\"img1\", \"img2\"], prob=0.3, gamma=(0.95, 1.05)), \n",
    "       \n",
    "    ])\n",
    "    val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"img1\",\"img2\"]),\n",
    "        EnsureChannelFirstd(keys=[\"img1\",\"img2\"]),\n",
    "        ScaleIntensityd(keys=[\"img1\",\"img2\"]),\n",
    "        Orientationd(keys=[\"img1\",\"img2\"],axcodes=\"RAS\"),\n",
    "        CropForegroundd(keys=[\"img1\",\"img2\"],source_key=\"img1\",allow_smaller=True),\n",
    "        Resized(keys=[\"img1\",\"img2\"], spatial_size=(130,170,130)),\n",
    "    ])\n",
    "    return train_transforms,val_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T02:57:44.780496Z",
     "iopub.status.busy": "2025-08-25T02:57:44.780244Z",
     "iopub.status.idle": "2025-08-25T02:57:44.795838Z",
     "shell.execute_reply": "2025-08-25T02:57:44.795136Z",
     "shell.execute_reply.started": "2025-08-25T02:57:44.780481Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_balanced_weights(num_samples_class0, num_samples_class1, smoothing_factor=0.1):\n",
    "    \n",
    "    total_samples = num_samples_class0 + num_samples_class1\n",
    "    \n",
    "    # 使用sqrt来减少极端类别不平衡的影响\n",
    "    weight_0 = torch.sqrt(torch.tensor(total_samples / (num_samples_class0 + smoothing_factor)))\n",
    "    weight_1 = torch.sqrt(torch.tensor(total_samples / (num_samples_class1 + smoothing_factor)))\n",
    "    \n",
    "    weights = torch.tensor([weight_0, weight_1])\n",
    "    \n",
    "    # 归一化权重到合理范围\n",
    "    min_weight = weights.min()\n",
    "    max_weight = weights.max()\n",
    "    weights = (weights - min_weight) / (max_weight - min_weight + 1e-6)\n",
    "    weights = weights * 0.9 + 0.1  # 确保最小权重为0.1\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T02:57:44.796792Z",
     "iopub.status.busy": "2025-08-25T02:57:44.796544Z",
     "iopub.status.idle": "2025-08-25T02:57:44.813191Z",
     "shell.execute_reply": "2025-08-25T02:57:44.812576Z",
     "shell.execute_reply.started": "2025-08-25T02:57:44.796777Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F  # for one-hot\n",
    "\n",
    "def custom_collate(batch):\n",
    "    collated = {}\n",
    "    collated['img1'] = torch.stack([item['img1'] for item in batch])\n",
    "    collated['img2'] = torch.stack([item['img2'] for item in batch])\n",
    "    \n",
    "    # --- FIX IS HERE ---\n",
    "    # The labels must be of type torch.long for CrossEntropyLoss\n",
    "    collated['label'] = torch.tensor([item['label'] for item in batch], dtype=torch.long)\n",
    "    \n",
    "    # extra: [age, gender_onehot(2), grade_onehot(3), IDH, MGMT, EOR] -> (B, 8)\n",
    "    ages = torch.tensor([item['age'] for item in batch], dtype=torch.float).unsqueeze(1)  # (B, 1)\n",
    "    genders = torch.tensor([item['gender'] for item in batch], dtype=torch.long)  # 假设 0=男, 1=女\n",
    "    gender_onehot = F.one_hot(genders, num_classes=2).float()  # (B, 2)\n",
    "    collated['extra'] = torch.cat([ages, gender_onehot], dim=1)  # (B, 1+2+3 = 6)\n",
    "    return collated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-25T02:58:07.409Z",
     "iopub.execute_input": "2025-08-25T02:57:44.814085Z",
     "iopub.status.busy": "2025-08-25T02:57:44.813882Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_fold(fold, trainlist, val_list, train_transforms, val_transforms, device, pin_memory):\n",
    "\n",
    "    train_ds = Dataset(data=trainlist, transform=train_transforms)\n",
    "    train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=4, pin_memory=pin_memory, collate_fn=custom_collate)\n",
    "    val_ds = Dataset(data=val_list, transform=val_transforms)\n",
    "    val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=1, pin_memory=pin_memory, collate_fn=custom_collate)\n",
    "\n",
    "   \n",
    "    model = HyCoSNet(\n",
    "        in_channels=1,\n",
    "        out_channels=2,  \n",
    "        swin_feature_size=24,\n",
    "        use_checkpoint=True\n",
    "    ).to(device)\n",
    "    \n",
    " \n",
    "    train_labels = [data['label'] for data in trainlist]\n",
    "    num_class_0 = train_labels.count(0)\n",
    "    num_class_1 = train_labels.count(1)\n",
    "    if num_class_0 == 0 or num_class_1 == 0:\n",
    "        class_weights = None\n",
    "    else:\n",
    "        w0 = (num_class_0 + num_class_1) / (2.0 * num_class_0)\n",
    "        w1 = (num_class_0 + num_class_1) / (2.0 * num_class_1)\n",
    "        class_weights = torch.tensor([w0, w1], dtype=torch.float32).to(device)\n",
    "\n",
    "   \n",
    "    weight_decay = 0.05\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=weight_decay)\n",
    "    loss_function = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\n",
    "    \n",
    "   \n",
    "    scaler = torch.amp.GradScaler('cuda')  # 修复 AMP 警告\n",
    "    accumulation_steps = 4\n",
    "\n",
    "   \n",
    "    max_epochs = 50\n",
    "    val_interval = 1\n",
    "    best_auc = -1\n",
    "    epoch_loss_values = []\n",
    "    auc_metric = ROCAUCMetric()\n",
    "    early_stop_patience = 20\n",
    "    early_stop_counter = 0\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for i, batch_data in tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Fold {fold} Epoch {epoch + 1}\"):\n",
    "            inputs0 = batch_data['img1'].to(device, non_blocking=True)\n",
    "            inputs1 = batch_data['img2'].to(device, non_blocking=True)\n",
    "            labels = batch_data['label'].to(device, non_blocking=True)\n",
    "            extra = batch_data['extra'].to(device, non_blocking=True)\n",
    "\n",
    "            with torch.amp.autocast('cuda'):  \n",
    "                outputs = model(inputs0, inputs1, extra)\n",
    "                loss = loss_function(outputs, labels)\n",
    "                loss = loss / accumulation_steps\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            epoch_loss += loss.item() * accumulation_steps\n",
    "\n",
    "            if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss /= len(train_loader)\n",
    "        epoch_loss_values.append(epoch_loss)\n",
    "        \n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            model.eval()\n",
    "            y_pred_list = []\n",
    "            y_list = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                with torch.amp.autocast('cuda', dtype=torch.float16):\n",
    "                    for val_data in val_loader:\n",
    "                        val_image0 = val_data[\"img1\"].to(device, non_blocking=True)\n",
    "                        val_image1 = val_data[\"img2\"].to(device, non_blocking=True)\n",
    "                        val_labels = val_data[\"label\"].to(device, non_blocking=True)\n",
    "                        val_extra = val_data['extra'].to(device, non_blocking=True)\n",
    "                        outputs = model(val_image0, val_image1, val_extra)\n",
    "                        y_pred_list.append(outputs)\n",
    "                        y_list.append(val_labels)\n",
    "            \n",
    "            y_pred = torch.cat(y_pred_list, dim=0)\n",
    "            y = torch.cat(y_list, dim=0)\n",
    "\n",
    "            y_pred_cpu = y_pred.cpu()\n",
    "            y_cpu = y.cpu()\n",
    "\n",
    "            post_pred = Compose([Activations(softmax=True)])\n",
    "            y_pred_processed = post_pred(y_pred_cpu)\n",
    "\n",
    "            y_cpu_with_channel = y_cpu.unsqueeze(1)\n",
    "            post_label = Compose([AsDiscrete(to_onehot=2, dim=1)])\n",
    "            y_true_processed = post_label(y_cpu_with_channel)\n",
    "            \n",
    "            auc_metric(y_pred_processed, y_true_processed)\n",
    "            auc_result = auc_metric.aggregate()\n",
    "            auc_metric.reset()\n",
    "\n",
    "            y_pred_class = y_pred_cpu.argmax(dim=1).numpy()\n",
    "            y_true_class = y_cpu.numpy()\n",
    "\n",
    "            acc = (y_pred_class == y_true_class).mean()\n",
    "            f1 = f1_score(y_true_class, y_pred_class, average='binary', zero_division=0)\n",
    "            recall = recall_score(y_true_class, y_pred_class, average='binary', zero_division=0)\n",
    "            \n",
    "            print(f\"Fold {fold} Epoch {epoch + 1}/{max_epochs} | Loss: {epoch_loss:.4f} | Val AUC: {auc_result:.4f} | Acc: {acc:.4f} | F1: {f1:.4f} | Recall: {recall:.4f}\")\n",
    "            \n",
    "            scheduler.step()\n",
    "\n",
    "            if auc_result > best_auc:\n",
    "                best_auc = auc_result\n",
    "                early_stop_counter = 0\n",
    "                torch.save(model.state_dict(), f\"/kaggle/working/best_model_fold_{fold}.pth\")\n",
    "                print(f\"--> New best model saved with AUC: {best_auc:.4f}\")\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "                print(f\"AUC did not improve. Early stopping counter: {early_stop_counter}/{early_stop_patience}\")\n",
    "\n",
    "            if early_stop_counter >= early_stop_patience:\n",
    "                print(f\"--- Early stopping triggered at epoch {epoch + 1} ---\")\n",
    "                break\n",
    "    return best_auc\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Clear GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Paths and initialization\n",
    "    from monai.utils import set_determinism\n",
    "    set_determinism(seed=42)\n",
    "    imgtype = ('_T1c_bias', '_FLAIR_bias')\n",
    "    pin_memory = True\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    " \n",
    "    fold_aucs = []\n",
    "    for fold in [4]:\n",
    "        print(f\"\\nStarting fold {fold}\")\n",
    "        trainlist = readIMGs(PATH_DATASET, df_train, imgtype, fold)\n",
    "        val_list = readIMGsTest(PATH_DATASET, df_train, imgtype, fold)\n",
    "        \n",
    "   \n",
    "        age_values = np.array([data['age'] for data in trainlist])\n",
    "        age_mean = age_values.mean()\n",
    "        age_std = age_values.std() + 1e-8\n",
    "        for data in trainlist + val_list:\n",
    "            data['age'] = (data['age'] - age_mean) / age_std\n",
    "\n",
    " \n",
    "        print(f\"Train samples: {len(trainlist)}, Val samples: {len(val_list)}\")\n",
    "        if len(val_list) < 2:\n",
    "            print(f\"Warning: Validation set too small for fold {fold}, AUC may be unreliable\")\n",
    "        else:\n",
    "            val_labels = [data['label'] for data in val_list]\n",
    "            print(f\"Val label distribution: 0={val_labels.count(0)}, 1={val_labels.count(1)}\")\n",
    "\n",
    "        train_transforms, val_transforms = transformers()\n",
    "        best_auc = train_fold(fold, trainlist, val_list, train_transforms, val_transforms, device, pin_memory)\n",
    "        fold_aucs.append(best_auc)\n",
    "    \n",
    "    print(f\"\\nFolds [0, 1] results: AUC = {fold_aucs}\")\n",
    "    print(f\"Mean AUC: {np.mean(fold_aucs):.4f} ± {np.std(fold_aucs):.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6950535,
     "sourceId": 11791512,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4431770,
     "sourceId": 12853190,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
